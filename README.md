# Alfred Agent Chat com PDF - Aplica√ß√£o de Consulta a Documentos PDF (v0.2.0)

Uma aplica√ß√£o Python que permite interagir com documentos PDF utilizando um modelo de linguagem local (LLM) via Ollama, com gerenciamento simplificado via Docker Compose e recupera√ß√£o de contexto aprimorada com estrat√©gias h√≠bridas de busca.

> **Nota:** Este projeto est√° em desenvolvimento ativo e pode conter "bugs". Testes e contribui√ß√µes s√£o bem-vindos!
> 
> **Novidades na v0.2.0:** Melhorias na extra√ß√£o de texto com PyMuPDFLoader, estrat√©gias h√≠bridas de recupera√ß√£o (Vector + BM25), gerenciamento de mem√≥ria para √≠ndices, integra√ß√£o completa com Ollama via Docker Compose, e suporte a GPU configur√°vel.

## Descri√ß√£o

Chat com PDF √© uma ferramenta que permite fazer upload de documentos PDF e realizar perguntas sobre seu conte√∫do, recebendo respostas detalhadas e contextualizadas. A aplica√ß√£o utiliza:

- **Processamento local** com Ollama para LLM
- **Embeddings** com HuggingFace (sentence-transformers)
- **Busca h√≠brida** combinando similaridade vetorial (FAISS) e BM25
- **Streaming de respostas** via Server-Sent Events (SSE)
- **Cache inteligente** para otimiza√ß√£o de performance

## Funcionalidades

- **Processamento 100% Local:** Todas as opera√ß√µes s√£o realizadas localmente via Ollama
- **API FastAPI:**
  - Upload de PDFs (`/upload-pdf/`)
  - Consultas com respostas via streaming (`/ask`)
  - Documenta√ß√£o interativa (Swagger UI)
- **Interface CLI:** Intera√ß√£o direta via linha de comando
- **Arquitetura Modular:**
  - Uso de princ√≠pios SOLID (SRP, inje√ß√£o de depend√™ncia)
  - Opera√ß√µes ass√≠ncronas para performance
  - Sistema de eventos para observabilidade
- **Estrat√©gias de Chunking:** Tokens, par√°grafos ou h√≠brida
- **Recupera√ß√£o H√≠brida:** Combina busca vetorial e BM25 para maior precis√£o
- **Cache Multin√≠vel:** Respostas, √≠ndices e servi√ßos
- **Suporte a GPU:** Configur√°vel para CUDA, MPS, NPU ou CPU
- **Testes Automatizados:** Estrutura completa com pytest

## Requisitos

- **Docker e Docker Compose** (recomendado)
- **Python 3.10+** (para desenvolvimento local)
- **Pelo menos 8GB de RAM** (16GB recomendado para modelos maiores)
- **GPU opcional:** NVIDIA CUDA, Apple MPS ou NPU para acelera√ß√£o

## Configura√ß√£o e Instala√ß√£o

### 1. Clone o Reposit√≥rio

```bash
git clone <seu-repositorio>
cd chat-with-pdf
```

### 2. Configure as Vari√°veis de Ambiente

Copie e configure o arquivo de ambiente:

```bash
cp .env.example .env
```

**Principais configura√ß√µes no `.env`:**

```bash
# Modelo Ollama (certifique-se que existe)
OLLAMA_MODEL_NAME="llama3.2"

# Configura√ß√£o de dispositivo para embeddings/computa√ß√£o
DEVICE_CONFIGURATION="cpu"  # Op√ß√µes: "cpu", "cuda", "mps", "npu"

# Configura√ß√µes de chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNKING_MODE="both"  # "tokens", "paragraphs", "both"

# Configura√ß√µes de recupera√ß√£o h√≠brida
INITIAL_VECTOR_K=50          # Documentos iniciais da busca vetorial
VECTOR_DISTANCE_THRESHOLD=1.0 # Limite de dist√¢ncia L2
FINAL_BM25_K=6              # Documentos finais do BM25

# Porta da API
UVICORN_PORT=8000
```

## Execu√ß√£o com Docker Compose (Recomendado)

### In√≠cio R√°pido

```bash
# 1. Construir e iniciar todos os servi√ßos
docker-compose up --build -d

# 2. Verificar se os servi√ßos est√£o rodando
docker-compose ps

# 3. Baixar o modelo LLM (substituir por seu modelo)
docker-compose exec ollama ollama pull llama3.2

# 4. Acessar a API
# http://localhost:8000/docs (Swagger UI)
```

### Estrutura dos Servi√ßos

O `docker-compose.yml` define dois servi√ßos principais:

- **`app`**: Aplica√ß√£o FastAPI principal
- **`ollama`**: Servidor Ollama para LLM

**Volumes persistentes:**
- `./pdfs:/app/pdfs` - Armazenamento de PDFs
- `./indices:/app/indices` - √çndices FAISS
- `${USERPROFILE}/.ollama/models:/root/.ollama/models` - Modelos Ollama

## Uso da Aplica√ß√£o

### API FastAPI

**Upload de PDF:**
```bash
curl -X POST "http://localhost:8000/upload-pdf/" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@documento.pdf"
```

**Fazer pergunta (streaming):**
```bash
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"pdf_filename": "documento.pdf", "question": "Qual √© o tema principal?"}'
```

### Interface CLI

```bash
# Executar CLI dentro do container
docker-compose exec app python -m src.cli.main

# Ou com desenvolvimento local
python -m src.cli.main
```

## Depura√ß√£o e Monitoramento

### Logs dos Servi√ßos

**Ver logs da aplica√ß√£o:**
```bash
# Logs em tempo real
docker-compose logs app -f

# Logs das √∫ltimas 100 linhas
docker-compose logs app --tail=100

# Filtrar logs por n√≠vel (ERROR, WARNING, INFO)
docker-compose logs app | grep ERROR
```

**Ver logs do Ollama:**
```bash
# Logs em tempo real do servi√ßo Ollama
docker-compose logs ollama -f

# Verificar inicializa√ß√£o do modelo
docker-compose logs ollama --tail=50

# Verificar se o modelo foi baixado com sucesso
docker-compose logs ollama | grep "pulled successfully"
```

**Logs de todos os servi√ßos:**
```bash
# Todos os logs em tempo real
docker-compose logs -f

# Logs espec√≠ficos por timestamp
docker-compose logs --since="2024-01-01T10:00:00"

# Salvar logs em arquivo para an√°lise
docker-compose logs > debug_logs.txt
```

### Comandos de Diagn√≥stico

**Verificar status dos servi√ßos:**
```bash
docker-compose ps

# Ver uso de recursos
docker stats

# Verificar se os containers est√£o saud√°veis
docker-compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}"
```

**Acessar shell do container:**
```bash
# Container da aplica√ß√£o
docker-compose exec app bash

# Container do Ollama
docker-compose exec ollama bash

# Executar comandos espec√≠ficos sem shell interativo
docker-compose exec app python --version
docker-compose exec ollama ollama --version
```

**Verificar modelos dispon√≠veis no Ollama:**
```bash
# Listar modelos instalados
docker-compose exec ollama ollama list

# Verificar tamanho dos modelos
docker-compose exec ollama du -sh /root/.ollama/models/*

# Buscar modelos dispon√≠veis online
docker-compose exec ollama ollama search llama
```

**Testar conectividade e APIs:**
```bash
# Testar conectividade com Ollama
docker-compose exec app curl http://ollama:11434/api/version

# Testar API da aplica√ß√£o
curl http://localhost:8000/docs

# Verificar endpoints dispon√≠veis
curl http://localhost:8000/openapi.json | jq .
```

### Resolu√ß√£o de Problemas Espec√≠ficos

**Problema: Script de inicializa√ß√£o falha com "curl: not found"**

O script `init-ollama.sh` foi atualizado para instalar automaticamente `curl` ou usar `wget` como fallback:

```bash
# Verificar logs de inicializa√ß√£o do Ollama
docker-compose logs ollama --tail=20

# Se ainda houver problemas, rebuildar o container
docker-compose down
docker-compose up --build ollama
```

**Problema: Modelo n√£o baixa automaticamente**

```bash
# Verificar se o modelo especificado existe
docker-compose exec ollama ollama search llama3.2

# Baixar manualmente se necess√°rio
docker-compose exec ollama ollama pull llama3.2

# Verificar espa√ßo em disco
docker-compose exec ollama df -h
```

**Problema: Timeout durante download do modelo**

```bash
# O script tem retry autom√°tico, mas voc√™ pode monitorar:
docker-compose logs ollama -f

# Para modelos grandes, considere baixar antes de subir os containers:
# 1. Subir apenas Ollama
docker-compose up -d ollama

# 2. Baixar modelo manualmente
docker-compose exec ollama ollama pull llama3.2

# 3. Subir o resto
docker-compose up -d app
```

## Configura√ß√µes Avan√ßadas do Dockerfile

### Vari√°veis de Ambiente Comentadas

No `Dockerfile`, h√° algumas vari√°veis de ambiente importantes comentadas:

```dockerfile
# Set environment variables for HuggingFace tokenizers and Python
# ENV TOKENIZERS_PARALLELISM="false"
# ENV PYTHONUNBUFFERED=1
# REMOVE the following line if you want GPU access:
# ENV CUDA_VISIBLE_DEVICES=""
```

#### Explica√ß√£o das Vari√°veis:

**1. `TOKENIZERS_PARALLELISM="false"`**
- **Prop√≥sito:** Desabilita paraleliza√ß√£o dos tokenizers do HuggingFace
- **Impacto na efici√™ncia:** 
  - ‚úÖ **Vantagem:** Evita deadlocks em ambientes containerizados
  - ‚ùå **Desvantagem:** Tokeniza√ß√£o mais lenta em textos muito grandes
  - üéØ **Recomenda√ß√£o:** Manter desabilitado em produ√ß√£o para estabilidade

**2. `PYTHONUNBUFFERED=1`**
- **Prop√≥sito:** For√ßa output imediato do Python (sem buffer)
- **Impacto na execu√ß√£o:**
  - ‚úÖ **Vantagem:** Logs aparecem imediatamente no Docker
  - ‚úÖ **Melhor depura√ß√£o:** Output em tempo real
  - ‚ùå **Overhead m√≠nimo:** Ligeiramente menos eficiente para I/O intensivo

**3. `CUDA_VISIBLE_DEVICES=""`**
- **Prop√≥sito:** Oculta GPUs do container quando definido como vazio
- **Impacto na infer√™ncia:**
  - ‚ö†Ô∏è **CPU for√ßado:** Mesmo com GPU dispon√≠vel, usar√° apenas CPU
  - üêå **Performance:** Embeddings e infer√™ncia significativamente mais lentos
  - üíæ **Mem√≥ria:** Menor uso de VRAM, mais uso de RAM
  - üîß **Compatibilidade:** Evita erros CUDA em ambientes sem suporte adequado

#### Recomenda√ß√µes de Configura√ß√£o:

**Para ambientes com GPU:**
```dockerfile
ENV TOKENIZERS_PARALLELISM="false"
ENV PYTHONUNBUFFERED=1
# Remover ou comentar a linha abaixo para usar GPU:
# ENV CUDA_VISIBLE_DEVICES=""
```

**Para ambientes apenas CPU:**
```dockerfile
ENV TOKENIZERS_PARALLELISM="false"
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=""  # For√ßa uso de CPU
```

### Configura√ß√£o de GPU no Docker Compose

Para usar GPU, descomente as se√ß√µes relevantes no `docker-compose.yml`:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1  # ou 'all' para todas as GPUs
          capabilities: [gpu]
```

## Desenvolvimento Local

### Configura√ß√£o sem Docker

1. **Instalar depend√™ncias:**
   ```bash
   pip install pipenv
   pipenv install --dev
   pipenv shell
   ```

2. **Configurar Ollama:**
   ```bash
   # Instalar Ollama localmente ou usar Docker apenas para Ollama
   docker-compose up -d ollama
   
   # Ou instalar Ollama nativamente e ajustar OLLAMA_HOST no .env
   # OLLAMA_HOST="http://localhost:11434"
   ```

3. **Executar aplica√ß√£o:**
   ```bash
   # API
   uvicorn src.api.main:app
   
   # CLI
   python -m src.cli.main
   ```

### Estrutura do Projeto

```
src/
‚îú‚îÄ‚îÄ api/           # FastAPI endpoints
‚îú‚îÄ‚îÄ cli/           # Interface linha de comando  
‚îú‚îÄ‚îÄ core/          # L√≥gica de neg√≥cios
‚îÇ   ‚îú‚îÄ‚îÄ services.py       # IndexService, QueryService
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py     # Cliente Ollama
‚îÇ   ‚îú‚îÄ‚îÄ prompt_builder.py # Constru√ß√£o de prompts
‚îÇ   ‚îú‚îÄ‚îÄ event_manager.py  # Sistema de eventos
‚îÇ   ‚îî‚îÄ‚îÄ observers.py      # Observadores de eventos (ex: Logging)
‚îú‚îÄ‚îÄ infra/         # Infraestrutura
‚îÇ   ‚îú‚îÄ‚îÄ pdf_repository.py      # Carregamento PDFs
‚îÇ   ‚îú‚îÄ‚îÄ vector_store_repository.py # Gerenciamento FAISS
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_factory.py     # Factory embeddings
‚îÇ   ‚îú‚îÄ‚îÄ chunk_strategies.py       # Estrat√©gias chunking
‚îÇ   ‚îî‚îÄ‚îÄ retriever_strategies.py   # Estrat√©gias recupera√ß√£o
‚îú‚îÄ‚îÄ config/        # Configura√ß√µes
‚îî‚îÄ‚îÄ utils/         # Utilit√°rios
```

## Testes

### Executar Testes

```bash
# Com Docker
docker-compose exec app pytest

# Desenvolvimento local
pipenv shell
pytest

# Com cobertura
pytest --cov=src --cov-report html
```

### Estrutura de Testes

- `tests/unit/` - Testes unit√°rios
- `tests/integration/` - Testes de integra√ß√£o API
- `tests/fixtures/` - Fixtures compartilhadas

## Solu√ß√£o de Problemas

### Problemas Comuns

**1. Ollama n√£o responde:**
```bash
# Verificar se est√° rodando
docker-compose exec ollama curl http://localhost:11434/api/version

# Restart do servi√ßo
docker-compose restart ollama
```

**2. Modelo n√£o encontrado:**
```bash
# Listar modelos
docker-compose exec ollama ollama list

# Baixar modelo
docker-compose exec ollama ollama pull llama3.2
```

**3. Erro de mem√≥ria:**
- Aumentar RAM dispon√≠vel para Docker
- Usar modelo menor (ex: `llama3.2:1b`)
- Ajustar `CHUNK_SIZE` e cache settings

**4. Performance lenta:**
- Verificar `DEVICE_CONFIGURATION` no `.env`
- Considerar usar GPU se dispon√≠vel
- Ajustar par√¢metros de recupera√ß√£o (`INITIAL_VECTOR_K`, `FINAL_BM25_K`)

## Contribui√ß√µes

Contribui√ß√µes s√£o muito bem-vindas! 

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commits seguindo conven√ß√µes
4. Abra um Pull Request

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa Apache-2.0. Consulte o arquivo `LICENSE` para mais detalhes.